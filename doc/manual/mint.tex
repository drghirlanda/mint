\documentclass[12pt,letterpaper]{memoir}
\usepackage{arev}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{array,graphicx,xspace}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\urlstyle{rm}
\usepackage[margin=2cm]{geometry}

\renewcommand{\familydefault}{\sfdefault}

\newcommand{\ls}[1]{\lstinline{#1}}

\chapterstyle{ell}
\setsecheadstyle{\Large\sffamily\raggedright}
\setsubsecheadstyle{\large\sffamily\raggedright}

\usepackage{listings}
\lstset{mathescape,language=C,basicstyle=\ttfamily,
  numbers=left,numberstyle=\tiny\sffamily,morekeywords={FILE}}

\let\fref\relax% memoir defines  \fref
\usepackage[plain]{fancyref}

\captiontitlefont{\small}
\setsecnumdepth{subsection}
\tightlists

\renewcommand{\thesection}{\arabic{chapter}.\arabic{section}}

\newcommand{\mint}{MINT\xspace}
\newcommand{\mintversion}{0.1\xspace}

\renewcommand{\vec}[1]{\ensuremath{\mathbf#1}\xspace}

\title{\bfseries The \mint cookbook}
\author{Stefano Ghirlanda}
\date{\footnotesize Version \mintversion of \today}

\makeindex

\newcommand{\note}[2]{\footnote{\textbf{#1:} #2}}

\begin{document}
\pagestyle{plain}

\maketitle

\clearpage

%\renewcommand{\printtoctitle}[1]{%
%  \large\bfseries\contentsname}
%\renewcommand{\cftsectionfont}{}
%\renewcommand{\cftsectionpagefont}{}
%\renewcommand{\cftchapterfont}{}
%\renewcommand{\cftchapterpagefont}{}
\tableofcontents

\renewcommand{\foottextfont}{\sffamily\footnotesize}

\renewcommand{\thepage}{\sffamily\arabic{page}}

\clearpage

\chapter{Is this for me?}
\label{chap:intro}

This short chapter provides a bird's eye view of the principal
features of \mint to help you decide whether it fits your needs.
\mint is a library for neural network simulations written in the C
programming language (specifically, C89). Using it requires writing C
programs (possibly very short ones) and simple text files to describe
your networks.

\section{\mint philosophy}
\label{sec:mint-philosophy}

\mint takes a ``natural scientist'' approach to neural network
simulations. These are viewed as a means to gain insight into the
operation of nervous systems, not as mathematical tools for
statistics, curve fitting, and so on. This perspective has affected
the design of \mint in a number of ways. If you are an engineer
looking for sophisticated supervised learning or network optimization
algorithms, \mint may not be the best tool for you.  If you are
interested in how biological nervous systems process inputs and
generate behavior, \mint is potentially for you.

In addition to a good number of built-in capabilities (which you can
read about in this manual), \mint provides you with a uniform
representation of neural networks (through the data structures
introduced in \fref{sec:the-very-basics} below) that enables you to
focus on the specifics of your particular neural network without
worrying about such things as memory management, data input and
output, and general properties of neural networks (e.g., that neural
networks are made of nodes wired together).  \mint, moreover, is
extensible.  With a few lines of code you can add a new neuron model
to \mint and use it as if it were one of those already in the
library. Ditto for weight models and learning algorithms. In many
cases you will be able to write a small C program once and for all,
and do a lot of work simply by changing a text file that describes
your network.

\section{What \mint does, and what it does not}
\label{sec:what-mint-does}

With \mint you can describe arbitrary neural networks compactly in
small text files. You can also add your own node and weight models and
using them seamlessly with the other \mint functions. You can specify
in detail what operations are to be performed to update a network.
\mint is less good for simulations of a very small number of very
detailed neurons (e.g., detailed 3D models of neurons). These are
possible, but you would end up doing most of the coding yourself with
relatively little benefit from the \mint infrastructure.  Here are
lists of currently available and unavailable features:\footnote{If you
  think that something should be added to either list, please contact
  us. REF}
\begin{description}
\item[Available:] general network topology, i.e., any number of node
  groups and weight matrices connected in any way (e.g., many weight
  matrices between two node groups), synchronous and asynchronous
  dynamics, arbitrary node and weight models (e.g., spiking or
  non-spiking neurons), ``global'' influences on neurons and weights
  (e.g., extracellular neurotransmitter concentrations), use images as
  input (requires the FreeImage library, see REF), use usb camera as
  input (on Linux, see REF), interface with motors and sensors on a
  Raspberry Pi computer (to build network controlled robots, see REF).
\item[Unavailable:] explicit positioning of neurons in 2D or 3D space,
  event-driven simulations and time delays in synaptic transmission,
  networked parallel programming (MPI is planned), modeling of neural
  morphology
\end{description}
The currently unavailable features may be implemented in the future.

\section{Space and speed}

From a technical point of view, \mint has a small footprint. It
compiles to under 100 KB on x86-64 (PC) and ARM processors and wastes
little memory. For example, the memory overhead of a weight matrix is
only 6 integers and 4 pointers.\footnote{By memory overhead we mean
  variables that \mint uses to keep track of things, rather than to
  hold information that is strictly part of the neural network, such
  as weight values.}  Some optimization also make \mint a good choice
if you need speed REF. Multi-threaded execution and sparse matrix
storage are also available. REF

\section{The very basics}
\label{sec:the-very-basics}

\mint describes neural networks in terms of five fundamental data
structures. The first three should be intuitive to anyone interested
in neural network simulations:
\begin{itemize}
\item \textit{Node groups}: Groups of simulated neurons
\item \textit{Weight matrices}: Simulated synaptic connections between
  nodes
\item \textit{Networks}: Collections of node groups and weight
  matrices
\end{itemize}
The last two data structures are specific to \mint and are used to
describe aspects of neural network operation:
\begin{itemize}
\item \textit{Operations} (``ops'' for short): Anything that can be
  done to a node group, a weight matrix, or a network
\item\textit{Spread schemes}: Representations of how the network
  is updated (how neural activation ``spreads'' through the network)
\end{itemize}
Examples of operations are initialization of weight matrices (e.g.,
setting weights at random), calculation of node output based on
received input, changes in weight matrices (``learning''), and so
on. Using these five data structures many kinds of neural networks can
be set up relatively simply and efficiently. The rest of this manual
introduces you to the data structures in some detail and walks you
through many examples of how \mint can be used in practice.

\section{Your first \mint network}
\label{sec:first-mint}

The easiest way to specify a network architecture in \mint is to write
a specially formatted text file. We use the file suffix
\lstinline{.arc} (for ``architecture file''), but you can use anything
you like.  A simple network with 10 input units connected to one
output unit with sigmoid nonlinearity is described as follows:

\lstinputlisting{../../example/starthere/starthere.arc} Line 1 starts

the network and line 2 specifies it is a feedforward
network.\footnote{\mint actually checks that this is true! The
  \lstinline{feedforward} specification influences how the network is
  updated, see REF.} Line 3 adds a first group of nodes called
``\lstinline{input}'' which contains 10 nodes. Line 4 adds a second
group of nodes, called ``\lstinline{output}'' with just 1 node and
identifies this node as having a \lstinline{sigmoid} transfer
function. Two parameters to configure the sigmoid function follow, see
REF SIGMOID PARAMETERS. Lastly, line 5 says that there is one weight
matrix connecting input to output, which is initialized upon network
creation with uniformly distributed numbers between $-1$ and $1$.
Assuming the file is called \lstinline{starthere.arc}, the following
program creates a network with the specified architecture:

\lstinputlisting{../../example/starthere/starthere.c} 

There are only 5 lines in this file that are specific to \mint:
\begin{itemize}
\item Line 1 reads the \lstinline{mint.h} header file.
\item Line 7 declares a pointer to a variable of type
  \lstinline{struct mint_network}, which holds all the information
  pertaining to the network.
\item Line 9 loads the network from file. Loading performs many error
  checks that ensure you end up with a valid network. If a network
  can't be loaded, \lstinline{mint_network_load} terminates the
  program with an error message.
\item Line 11 displays the network to the C standard output (e.g.,
  terminal window). You can also use this function to save a network
  to a file for later analysis or loading into another program.
\item Line 12 deletes the network (frees up the memory the network is
  using).
\end{itemize}
You can find this program and the architecture file above under
\lstinline{example/starthere} in the \mint source distribution. If you
run the program you will get something like:\footnote{The $\backslash$
  character on line 13 indicates a line break we inserted here, and
  will not be part of the output.}
\begin{lstlisting}
network
feedforward 
nodes input
size 10 states 0 
0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
nodes output
size 1 sigmoid 0.1 1 states 0 
0 
0 
weights input-output
random -1 1 1 states 0 cols 10 rows 1 
-0.155077 0.559366 0.669266 -0.974252 -0.192495 \
-0.238614 0.357579 -0.710728 0.0285505 -0.562432 
spread
input input-output output 
\end{lstlisting}
This includes everything we had in the initial file plus additional
information necessary to recreate an exact copy of the network:
\begin{itemize}
\item Lines 3--6 detail the \lstinline{input} node group, adding the
  information that these nodes have 0 internal state variables (the
  default, see REF to learn about state variables) and also listing
  the value of node inputs (line 5) and node outputs (line 6). These
  are all zeros because we have never actually used the network.
\item Lines 7--10 detail the \lstinline{output} node group in the same
  way.
\item Lines 11--14 detail the \lstinline{input-output} weight matrix,
  stating explicitly its dimensions (\lstinline{rows} and
  \lstinline{cols}) and the fact that the weights have no internal
  state variables (see REF). Lines 9--10 list the values that the
  weights received from the random initialization. If this file is
  loaded again, these values will be used and the initialization will
  \textit{not} be performed. This ensures that you can load an exact
  copy of the network.\footnote{It is also possible to re-initialize
    the weights at random, see REF.}
\item Lines 15--16 detail how the network is updated every time that
  the function \lstinline{mint_network_operate} is used (it was not in
  our sample program. This information consists of the keyword
  \lstinline{spread} followed by a list of weight matrix and node
  group names. In this case, we see that the \lstinline{input} node
  group is updated first, then the \lstinline{input-output} matrix is
  used to propagate \lstinline{input} activity to \lstinline{output}
  nodes, and finally \lstinline{output} nodes are updated. This
  information has been generated by the \lstinline{feedforward}
  specification. Each spread step may involve several operations
  depending on how the network has been setup. In this example, for
  instance, \lstinline{output} nodes are updated using the sigmoid
  function specified on line 8. Setting up network spread is a key
  component of neural network simulation in \mint, and is detailed
  REF.
\end{itemize}

\section{Where to go from here}

The next chapter 
\chapter{How to get \mint, and what you get}
\label{sec:install}

\section{How}

---EMPTY UNTIL FIRST PUBLIC RELEASE--- 

\section{What}

The \mint distribution contains the following files and folders:
\begin{itemize}
\item \lstinline{doc/}: Documentation, consisting of this manual and a
  set of HTML pages that document the \mint source code.
\item \lstinline{src/}: The C source for \mint.
\item \lstinline{examples/}: Numerous examples of various features.
\item A few files used to install \mint, see below.
\item \lstinline{README}: A brief file pointing you to this document.
\end{itemize}

\section{Installation}


\chapter{Data Structures}
\label{cha:mint-data-structures}

\mint is organized around five variable types:
\begin{enumerate}
\item \lstinline{mint_nodes} represent groups of neurons.
\item \lstinline{mint_weights} represent synaptic connections between
  groups of neurons.
\item A \lstinline{struct mint_network} represents a neural networks.
\item A \lstinline{mint_op} represent an operation that can be
  performed on the previous three types.
\item A \lstinline{mint_spread} holds information on how a network is
  updated.
\end{enumerate}
You will usually deal with nodes, weights, and networks. Occasionally
you will want to work with ops, very rarely you will have to use
spreads in your C code.

Neural network computations are implemented as operations on nodes,
weights, and networks. These operations are programmed through
functions with a specific interface (see REF) and they can all be
specified in network architecture files. For example, we saw in
\ref{sec:first-mint} that the \lstinline{sigmoid} operation instructs
mint to calculate node output as a sigmoid function of node input.

\section{The neural population model}

\mint defines a type \lstinline{mint_nodes} to manage population of
model neurons. A variable of type \lstinline{mint_nodes} is referred
to as a ``node group.'' A node group is typically created from a
description contained in a text file, such as:
\begin{lstlisting}
nodes n1 size 100
\end{lstlisting}
where \lstinline{n1} is the name of the group. This line creates a node group with 100 neurons. Creating the node group from file is done like this:
\begin{lstlisting}
mint_nodes n;
file = fopen( ``nodes.arc'', ``r'' );
n = mint_nodes_load( file );
fclose( file );
\end{lstlisting}
The variable \lstinline{n} can now be accessed as a two-dimensional
array of \lstinline{float}s with the following semantics:\footnote{As
  it is usual in C, there is no automatic check that array indices
  make sense, although all \mint functions perform such checks.}
\begin{itemize}
\item \lstinline{n[0][i]} is the input received by node \lstinline{i}
  (\lstinline{0 $\le$ i $<$ size})
\item \lstinline{n[1][i]} is the output of node \lstinline{i}.
\end{itemize}

It is also possible to endow nodes with additional state
variables. The purpose of such variables is to enable programming of
neuron models whose dynamics depends on more than input and output,
e.g., spiking neuron models in which a 0/1 output is determined based
on a continuous membrane potential. In such a case, node output would
be set to be 0 or 1 according to the value of a state variable
modeling membrane potential (see REF for an example). States are added
like this:
\begin{lstlisting}
nodes n1 size 100 states 1
\end{lstlisting}
which would add one state variable, accessed in C code as
\lstinline{n[2][i]}. That is, the $j$-th state variable of node $i$ is
\lstinline{n[j+1][i]}.

Many other directives can be added to a node group declaration. The
following line, for example, builds a node group in which each node
receives normally distributed noise in input (in addition to the input
received from other nodes in the network) and computes output from input using a logistic function:
\begin{lstlisting}
nodes n1 size 100 noise 0 0 0.1 logistic
\end{lstlisting}
The three parameters following \lstinline{noise} mean that the noise
is added to variable 0 (node input) with a mean of 0 and standard
deviation of 0.1. The \lstinline{noise} and \lstinline{operations} are
applied in the order given. Writing \lstinline{logistic noise 0 0 0.1}
would result in the logistic function being applied before the noise
is added to node input. The noise would therefore have no effect. It
is possible to add noise to node output by changing the noise target
variable: \lstinline{logistic noise 1 0 0.1}. As a further example,
the following line declares spiking nodes with spontaneous activity of
5 spikes/s:\footnote{``Per second'' implies that each node update is
  taken to correspond to 1 ms of real time. A more precise statement
  is that \lstinline{spikes 5} results in a node being turned on
  (firing) with probability $5/1000$ for every update.}
\begin{lstlisting}
nodes n1 size 100 spikes 5
\end{lstlisting}
In summary, \mint defines a number of common operations on nodes that
can be used by declaring them in a configuration file (see REF for a
complete list). Additionally, \mint provides a framework for adding
node operations in a way that integrates seamlessly with the rest of
the library (see REF).

Lastly, everything that can be done by writing and loading
configuration files can also be done directly in C code, but in a more
tedious way.

\section{The synaptic connection model}

\mint defines a type \lstinline{mint_weights} to manage synaptic
connections between node groups. A variable of type \ls{mint_weights}
is referred to as a ``weight matrix.'' It can be created in two ways
from an architecture file. The first is by providing explicitly its
dimensions:
\begin{lstlisting}
weights w rows 50 cols 10
\end{lstlisting}
This would create a weight matrix named \ls{w}, with appropriate
dimensions to the output of a node group of 10 nodes (the number of
columns) to a node group of 50 nodes (the number of rows). Creating
the weight matrix is similar to creating a node group:
\begin{lstlisting}
mint_weights w;
file = fopen( ``weights.arc'', ``r'' );
w = mint_weights_load( file );
fclose( file );
\end{lstlisting}
A weight matrix created like this, however, would have to be added
manually to a neural network and connected to node groups to several
function calls. This is generally unnecessary as you can define weight
matrices directly as part of a neural network (this is, after all,
where they do their job), like this:
\begin{lstlisting}
network
nodes n1 size 10
nodes n2 size 50
weights w n1-n2
\end{lstlisting}
The \ls{network} keyword introduces a neural network (see REF). Then
two node groups are defined, and lastly a weight matrix is defined
connecting node groups \ls{n1} and \ls{n2}. The first node group is
the \textit{pre-synaptic one}, i.e., the one sending output, and the
second is the \textit{post-synaptic} one, i.e., the one receiving
input. In other words, the synaptic connections modeled by \ls{w}
would be between the axons of \ls{n1} nodes and the dendrites of
\ls{n2} nodes.\footnote{Can you have connections between dendrites or
  other departures from the ``from axon to dendrite'' paradigm in
  \mint? Yes. \mint does not enforce any particular neurobiological
  interpretation of connections and thus you are free to model any
  kind of neurobiological interaction between neurons through \mint
  weight matrices. See REF for tips on how to do this.} This way of
defining weight matrices lets \mint take care of bookkeeping such as
what are the appropriate dimensions for matrix \ls{w} and which nodes
it connects. The whole network can be then loaded as detailed below
(and as already seen in \fref{sec:first-mint}).

Once a weight matrix is created (let's suppose for simplicity that it
is stored in the variable \ls{w}), it can be accessed as a
three-dimensional array of \ls{float}s in which \ls{w[0][i][j]} is the
value of the synaptic connection \textit{from} node $j$ in the
pre-synaptic node group \textit{to} node $i$ in the post-synaptic
one. The array is three-dimensional rather than two-dimensional to
enable the use of additional state variables of synaptic connections,
as we have seen for node groups. State variables can be used to model
the internal dynamics of synapses. Suppose, for instance, that you
want to take into account that neurotransmitters can deplete and need
to be replenished. A simple way of doing so is do have a state
variable that represents the amount of available neurotransmitter, and
set up a weight

although Node groups can be used as
two-dimensional arrays of floats with special semantics. Let's see an
example. Nodes The building blocks of a \mint network are:
\begin{description}
\item[Node groups:] They model groups of neurons. They are variables
  of type \lstinline{mint_nodes} and appear to the user as
  two-dimensional arrays of \lstinline{float}s. The first index refers
  to a \textit{state variable} of the nodes, the second is the node
  index. Two state variables are automatically created for each node
  group: input and output. Thus, if that \lstinline{n} is a node
  group, \lstinline{n[0][i]} is the input received by the $i$-th node
  and \lstinline{n[1][i]} is its output. Other state variables can be
  created as needed, see typically to endow the nodes with specific
  properties (\fref{sec:transfer-functions}.
\item[Weight matrices:] They model synapses between neurons and other
  ways neurons interact (see \fref{sec:hormones-etc},
  \fref{sec:body-states}). They are variables of type
  \lstinline{mint_weights} and look to the user as three-dimensional
  arrays of \lstinline{float}s. The first index refers to a state
  variable of the weights, the second the row index and the third the
  column index in the weight matrix. One state variable is
  automatically created and represents the weight value. Thus, if
  \lstinline{w} is a weight matrix, \lstinline{w[0][r][c]} is the
  value of the weight in row $r$ and column $c$ of the matrix. Other
  state variables can be created as needed.
\item[Update functions] determine how nodes change their activity in
  response to input, and how weights change their values (how they
  \textit{learn}). Each node group and each weight matrix is given an
  update function, unless we do not need to update them (e.g., fixed
  weights).
\item[Networks] are composed of node groups connected by weight
  matrices, linked together by a \textit{spread scheme}, which is a
  description of the operations that are performed when the network is
  updated. They are variables of type \lstinline{struct mint_network}.
\end{description}

Typically, you will create one or more \mint networks by specifying
the properties of a number of node groups (their size, number of
states, activation function, etc.) and how they are connected via
weight matrices. You may also specify how the network is updated
(synchronouse or asynchronous dynamics, feed-forward spreading of
activation, etc.).


\chapter{Cookbook}
\label{chap:cookbook}

\section{How do i create a feed-forward network?}
\label{sec:feed-forward}


\section{How do I create a recurrent network?}
\label{sec:recurrent}

\section{How do I run a network?}
\label{sec:running-network}

\section{How can a network learn?}
\label{sec:learning}

\section{How do I choose node transfer functions?}
\label{sec:transfer-functions}

\section{How do I set learning rules?}
\label{sec:learning-rules}

\section{How do I set up how a network is updated?}
\label{sec:spreading}

\section{How do I use images as network stimuli?}
\label{sec:images}

\section{How do I simulate a sense organ?}
\label{sec:sense-organ}

\section{How do I simulate hormones, neuromodulators, etc.?}
\label{sec:hormones-etc}

\section{How do I simulate  hunger, thirst, and other body states?}
\label{sec:body-states}

\section{How do I simulate spiking neurons?}
\label{sec:spiking}

\section{How do I simulate continuous time?}
\label{sec:continuous-time}

\section{How do I control a robot or some kind of hardware with a network?}
\label{sec:hardware-control}

\section{How do I visualize networks, node groups and weight
  matrices?}
\label{sec:network-analysis}

\section{Input file syntax}

\subsection{Node model}
\label{sec:nodemodel}
A \mint node is characterized by an input $y$, an output $z$ and
optionally a state vector $\vec s$, with any number of components.
Node inputs may be set by the user (e.g.\ to simulate the reception of
a stimulus) or arise from the weighted outputs of other nodes (via
weight matrices). Node output is determined from node input and node
state by an arbitrary function:
\begin{equation}
  \label{eq:node-model}
  z = f( y, \vec s, \vec g )
\end{equation}
This operation is a called a \textit{node update}. The spreading
scheme is the sequence of matrix-vector multiplications and node
update operations whereby network state is updated (see
\fref{sec:spreading}). In addition to affecting node output, an update
may have other effects such as modifying node state.

\subsection{Weight model}
\label{sec:weightmodel}
A weight is characterized by a value $w$ and a state vector $\vec s$,
with 0 or more components. A weight can be updated in much the same
way as a node, though weight update is usually called ``learning.''
In \mint, an arbitrary function can be supplied to update weights
based on current value, current state and the current input, output
and state variables of the nodes joined by a weight.

\section{Node update functions}
\label{sec:nupdate}

\subsection{Using update functions provided with \mint}
\label{sec:update-using}

See \lstinline{nop.h}.

\subsection{Adding update functions}
\label{sec:update-adding}

Adding an update function to \mint requires two steps:
\begin{enumerate}
\item writing the update function itself;
\item letting \mint know about the function.
\end{enumerate}
These operations do not require modifications to the \mint source.
The prototype of a node update function is
\begin{lstlisting}
void function( mint_nodes n, int min, int max, float *p );
\end{lstlisting}
where
\begin{itemize}
\item \lstinline{n} is the node object to update;
\item \lstinline{min}-\lstinline{max} is the range of nodes that the
  function should update;
\item \lstinline{p} is a vector of user-configurable parameters that
  may modify the operation of the update function 
\end{itemize}
The basic structure of an  update rule is very simple:\footnote{The
  \lstinline{for} loop assumes that the same operation is performed to
update all nodes. Different operations, however, may be performed if
desired.}
\begin{lstlisting}
void function( mint_nodes n, int min, int max, float *p ) {
  int i;
  /* retrieve update function parameters from node object */
  for( i=min; i<max; i++ ) {
    /* update node i */
  }
}  
\end{lstlisting}
For instance, imagine we want to update nodes as follows: if the input
is above a threshold, the output should be set to 1, otherwise it
should be set to 0. We store the threshold value as parameter 0 of the
rule (we will see below how to tell \mint about the number of
parameters taken by a rule). The implementation of the rule would be:
\begin{lstlisting}
void threshold_node( mint_nodes n, int min, int max, float *p ) {
  int i;
  float threshold;
  threshold = p[0];
  for( i=min; i<max; i++ ) {
    if( n[0][i] > threshold )
      n[1][i] = 1.;
    else
      n[1][i] = 0.;
  }
}  
\end{lstlisting}
Note that the function specifies an update mechanism (the threshold
mechanism), but does not determine the threshold value itself, which
is stored as a parameter in the node object.  Note also that the
function need not check that \lstinline{min} and \lstinline{max} are
sensible values for this node object, \mint checks this (in debug
mode, see \fref{sec:debugging}).

To be used with \mint, the function must be registered with a call
to
\begin{lstlisting}
void mint_op_add( const char *name, int type, void *f, 
                       int nparam, float *param );
\end{lstlisting}
Thus for the above function:
\begin{lstlisting}
  float p[1] = { 1. };
  mint_op_add( "threshodl", mint_op_nodes_update, threshold_node, 
               1, p );
\end{lstlisting}
The values provided for the parameter(s) become the default values for
the function, but can be changed as needed (see \lstinline{update.h}).

\section{Spreading schemes}
\label{sec:spreading}

A spreading scheme defines a sequence of matrix-vector multiplications
and node updates.


\section{Debugging}
\label{sec:debugging}

Compiling \mint the \lstinline{DEBUG} flag turns on a number of
checks, ensuring for instance that indices supplied as function
parameters are in the proper range, and that object copies operate on
objects with the same ``geometry'' (e.g.\ node groups with the same
size and number of states).  If a check fails, the program displays a
hopefully informative message and calls \lstinline{abort()}.
\Fref{sec:install} explains how to compile a debugging version of
\mint.


\section{Replacing the stock matrix-vector multiplication}
\label{sec:mvm}

\chapter{Space and speed}
\label{chap:details}

The major efficiency considerations in \mint is not having data
structures for single nodes and weights, but rather for node groups
and weight matrices directly, and caring for the memory layout of
data. The benefits are seen mostly in the function that performs
matrix-vector multiplication. A bare-bones such function would be:
\begin{lstlisting}
void matrix_vector_multiply( mint_weights w, mint_nodes from, mint_nodes to ) {
  int i, j, r, c;
  c = mint_weights_cols( w );
  r = mint_weights_rows( w );
  for( i=0; i<r; i++ ) for( j=0; j<c; j++ ) 
    to[0][i] += w[0][i][j] * from[1][j];
}  
\end{lstlisting}
Due to the memory layout of nodes and weights, all memory accesses are
sequential in the above \lstinline{for} loops. Indeed, the actual code
for matrix-vector multiplication in \mint is slightly longer to take
advantage of this memory layout:\footnote{The \lstinline{float *p}
  argument comes from the general form of \mint ops, and is not used
  here (see REF).}

\lstinputlisting[firstline=36,lastline=50]{../../src/wop.c}

In lines 7, 8, and 10 we set up pointers to the first weight value,
the first target variable of \lstinline{to} nodes,\footnote{By
  default, the target is 0, i.e., node input as in the previous
  listing, but can be changed if desired, see REF.} and the first
output value of \lstinline{from} nodes. Then in the for loops we
simply increment these pointers because we know that all memory
locations are contiguous. The C compiler, on the other hand, cannot do
this because it does not know that \lstinline{w[0][1]} points to a
location that is next to \lstinline{w[0][0][cols]}. Thus the compiler
has to generate additional instructions to look-up memory addresses.

Efficiency considerations have also influenced the design of node and
weight update functions. The approach that first comes to mind to
update a node group is something like (pseudo-code):
\begin{lstlisting}
for( i=0; i<n; i++ )
  update_the_node( node_group_object, i );
\end{lstlisting}
where the function call updates node $i$ of the node group. The
drawback is that there are $n$ function calls. In \mint, an update
function takes a \textit{range} as argument, telling it which nodes to
update. Thus the loop is inside the update function, saving $n-1$
function calls. In other words, in \mint we have 
(pseudo-code):
\begin{lstlisting}
  update_node_range( node_group_object, imin, imax );
\end{lstlisting}
where \lstinline{update_node_range()} does both the looping and the
node updating.\footnote{The update function cannot be inlined because
  it can be configured by the user, hence the compiler cannot know
  which function to inline.} The same approach is taken for weight
updates.

\section{Benchmarks}

\clearpage

\printindex

\end{document}

