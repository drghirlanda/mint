\documentclass[12pt,letterpaper]{memoir}
\usepackage{arev}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{array,graphicx,xspace}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\urlstyle{rm}
\usepackage[margin=2cm]{geometry}

\renewcommand{\familydefault}{\sfdefault}

\chapterstyle{ell}
\setsecheadstyle{\Large\sffamily\raggedright}
\setsubsecheadstyle{\large\sffamily\raggedright}

\usepackage{listings}
\lstset{language=C,basicstyle=\ttfamily,
  numbers=left,numberstyle=\tiny\sffamily,morekeywords={FILE}}

\let\fref\relax% memoir defines  \fref
\usepackage[plain]{fancyref}

\captiontitlefont{\small}
\setsecnumdepth{subsection}
\tightlists

\renewcommand{\thesection}{\arabic{chapter}.\arabic{section}}

\newcommand{\mint}{{\scshape\sffamily mint}\xspace}
\newcommand{\mintversion}{0.1\xspace}

\renewcommand{\vec}[1]{\ensuremath{\mathbf#1}\xspace}

\title{\bfseries The \mint cookbook}
\author{Stefano Ghirlanda}
\date{\footnotesize Version \mintversion of \today}

\makeindex

\newcommand{\note}[2]{\footnote{\textbf{#1:} #2}}

\begin{document}
\pagestyle{plain}

\maketitle

\renewcommand{\printtoctitle}[1]{%
  \large\bfseries\contentsname}
\renewcommand{\cftsectionfont}{}
\renewcommand{\cftsectionpagefont}{}
\renewcommand{\cftchapterfont}{}
\renewcommand{\cftchapterpagefont}{}
\tableofcontents*

\renewcommand{\foottextfont}{\sffamily\footnotesize}

\renewcommand{\thepage}{\sffamily\arabic{page}}

\clearpage

\chapter{Is this for me?}
\label{chap:intro}

This short chapter provides a bird's eye view of the principal
features of \mint to help you decide whether it fits your needs.
\mint is a library for neural network simulations written in the C
programming language (specifically, C89). Using it requires writing C
programs (possibly very short ones) and simple text files to describe
your networks.

\section{\mint philosophy}
\label{sec:mint-philosophy}

\mint takes a ``natural scientist'' approach to neural network
simulations. These are viewed as a means to gain insight into the
operation of nervous systems, not as mathematical tools for
statistics, curve fitting, and so on. This perspective has affected
the design of \mint in a number of ways. If you are an engineer
looking for sophisticated supervised learning or network optimization
algorithms, \mint may not be the best tool for you.  If you are
interested in how biological nervous systems process inputs and
generate behavior, \mint is potentially for you.

In addition to a good number of built-in capabilities (which you can
read about in this manual), \mint provides you with a uniform
representation of neural networks (through the data structures
introduced in \fref{sec:the-very-basics} below) that enables you to
focus on the specifics of your particular neural network without
worrying about such things as memory management, data input and
output, and general properties of neural networks (e.g., that neural
networks are made of nodes wired together).  \mint, moreover, is
extensible.  With a few lines of code you can add a new neuron model
to \mint and use it as if it were one of those already in the
library. Ditto for weight models and learning algorithms. In many
cases you will be able to write a small C program once and for all,
and do a lot of work simply by changing a text file that describes
your network.

\section{What \mint does, and what it does not}
\label{sec:what-mint-does}

With \mint you can describe arbitrary neural networks compactly in
small text files. You can also add your own node and weight models and
using them seamlessly with the other \mint functions. You can specify
in detail what operations are to be performed to update a network.
\mint is less good for simulations of a very small number of very
detailed neurons (e.g., detailed 3D models of neurons). These are
possible, but you would end up doing most of the coding yourself with
relatively little benefit from the \mint infrastructure.  Here are
lists of currently available and unavailable features:\footnote{If you
  think that something should be added to either list, please contact
  us. REF}
\begin{description}
\item[Available:] general network topology, i.e., any number of node
  groups and weight matrices connected in any way (e.g., many weight
  matrices between two node groups), synchronous and asynchronous
  dynamics, arbitrary node and weight models (e.g., spiking or
  non-spiking neurons), ``global'' influences on neurons and weights
  (e.g., extracellular neurotransmitter concentrations), use images as
  input (requires the FreeImage library, see REF), use usb camera as
  input (on Linux, see REF), interface with motors and sensors on a
  Raspberry Pi computer (to build network controlled robots, see REF).
\item[Unavailable:] explicit positioning of neurons in 2D or 3D space,
  event-driven simulations and time delays in synaptic transmission,
  networked parallel programming (MPI is planned)
\end{description}
The currently unavailable features may be implemented in the future.

\section{Space and speed}

From a technical point of view, \mint has a small footprint. It
compiles to under 100 KB on x86-64 (PC) and ARM processors and wastes
little memory. For example, the memory overhead of a weight matrix is
only 6 integer variables and 4 pointers.\footnote{By memory overhead
  we mean variables that \mint uses to keep track of things, rather
  than to hold information that is strictly part of the neural
  network, such as weight values.}  Some optimization also make \mint
a good choice if you need speed REF. Multi-threaded execution and
sparse matrix storage are also available. REF

\section{The very basics}
\label{sec:the-very-basics}

\mint describes neural networks in terms of five fundamental data
structures. The first three should be intuitive to anyone interested
in neural network simulations:
\begin{itemize}
\item \textit{Node groups}: Groups of simulated neurons
\item \textit{Weight matrices}: Simulated synaptic connections between
  nodes
\item \textit{Networks}: Collections of node groups and weight
  matrices
\end{itemize}
The last two data structures are specific to \mint and are used to
describe aspects of neural network operation:
\begin{itemize}
\item \textit{Operations} (``ops'' for short): Anything that can be
  done to a node group, a weight matrix, or a network
\item\textit{Spread schemes}: Representations of how the network
  is updated (how neural activation ``spreads'' through the network)
\end{itemize}
Examples of operations are initialization of weight matrices (e.g.,
setting weights at random), calculation of node output based on
received input, changes in weight matrices (``learning''), and so
on. Using these five data structures many kinds of neural networks can
be set up relatively simply and efficiently. The rest of this manual
introduces you to the data structures in some detail and walks you
through many examples of how \mint can be used in practice.

\section{Your first \mint network}
\label{sec:first-mint}

The easiest way to specify a network architecture in \mint is to write
a specially formatted text file. We use the file suffix
\lstinline{.arc} (for ``architecture file''), but you can use anything
you like.  A simple network with 10 input units connected to one
output unit with sigmoid nonlinearity is described as follows:

\lstinputlisting{../../example/starthere/starthere.arc} Line 1 starts

the network and line 2 specifies it is a feedforward
network.\footnote{\mint actually checks that this is true! The
  \lstinline{feedforward} specification influences how the network is
  updated, see REF.} Line 3 adds a first group of nodes called
``\lstinline{input}'' which contains 10 nodes. Line 4 adds a second
group of nodes, called ``\lstinline{output}'' with just 1 node and
identifies this node as having a \lstinline{sigmoid} transfer
function. Two parameters to configure the sigmoid function follow, see
REF SIGMOID PARAMETERS. Lastly, line 5 says that there is one weight
matrix connecting input to output, which is initialized upon network
creation with uniformly distributed numbers between $-1$ and $1$.
Assuming the file is called \lstinline{starthere.arc}, the following
program creates a network with the specified architecture:

\lstinputlisting{../../example/starthere/starthere.c} 

There are only 5 lines in this file that are specific to \mint:
\begin{itemize}
\item Line 1 reads the \lstinline{mint.h} header file.
\item Line 7 declares a pointer to a variable of type
  \lstinline{struct mint_network}, which holds all the information
  pertaining to the network.
\item Line 9 loads the network from file. Loading performs many error
  checks that ensure you end up with a valid network. If a network
  can't be loaded, \lstinline{mint_network_load} terminates the
  program with an error message.
\item Line 11 displays the network to the C standard output (e.g.,
  terminal window). You can also use this function to save a network
  to a file for later analysis or loading into another program.
\item Line 12 deletes the network (frees up the memory the network is
  using).
\end{itemize}
You can find this program and the architecture file above under
\lstinline{example/starthere} in the \mint source distribution. If you
run the program you will get something like:\footnote{The $\backslash$
  character on line 13 indicates a line break we inserted here, and
  will not be part of the output.}
\begin{lstlisting}
network
feedforward 
nodes input
size 10 states 0 
0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
nodes output
size 1 sigmoid 0.1 1 states 0 
0 
0 
weights input-output
random -1 1 1 states 0 cols 10 rows 1 
-0.155077 0.559366 0.669266 -0.974252 -0.192495 \
-0.238614 0.357579 -0.710728 0.0285505 -0.562432 
spread
input input-output output 
\end{lstlisting}
This includes everything we had in the initial file plus additional
information necessary to recreate an exact copy of the network:
\begin{itemize}
\item Lines 3--6 detail the \lstinline{input} node group, adding the
  information that these nodes have 0 internal state variables (the
  default, see REF to learn about state variables) and also listing
  the value of node inputs (line 5) and node outputs (line 6). These
  are all zeros because we have never actually used the network.
\item Lines 7--10 detail the \lstinline{output} node group in the same
  way.
\item Lines 11--14 detail the \lstinline{input-output} weight matrix,
  stating explicitly its dimensions (\lstinline{rows} and
  \lstinline{cols}) and the fact that the weights have no internal
  state variables (see REF). Lines 9--10 list the values that the
  weights received from the random initialization. If this file is
  loaded again, these values will be used and the initialization will
  \textit{not} be performed. This ensures that you can load an exact
  copy of the network.\footnote{It is also possible to re-initialize
    the weights at random, see REF.}
\item Lines 15--16 detail how the network is updated every time that
  the function \lstinline{mint_network_operate} is used (it was not in
  our sample program. This information consists of the keyword
  \lstinline{spread} followed by a list of weight matrix and node
  group names. In this case, we see that the \lstinline{input} node
  group is updated first, then the \lstinline{input-output} matrix is
  used to propagate \lstinline{input} activity to \lstinline{output}
  nodes, and finally \lstinline{output} nodes are updated. This
  information has been generated by the \lstinline{feedforward}
  specification. Each spread step may involve several operations
  depending on how the network has been setup. In this example, for
  instance, \lstinline{output} nodes are updated using the sigmoid
  function specified on line 8. Setting up network spread is a key
  component of neural network simulation in \mint, and is detailed
  REF.
\end{itemize}


\chapter{How to get \mint, and what you get}
\label{sec:install}

\section{How}

---EMPTY UNTIL FIRST PUBLIC RELEASE--- 

\section{What}

The \mint distribution contains the following files and folders:
\begin{itemize}
\item \lstinline{doc/}: Documentation, consisting of this manual and a
  set of HTML pages that document the \mint source code.
\item \lstinline{src/}: The C source for \mint.
\item \lstinline{examples/}: Numerous examples of various features.
\item A few files used to install \mint, see below.
\item \lstinline{README}: A brief file pointing you to this document.
\end{itemize}

\section{Installation}


\chapter{\mint data structures}
\label{cha:mint-data-structures}

\mint is organized around five variable types:
\begin{enumerate}
\item \lstinline{mint_nodes} represent groups of neurons.
\item \lstinline{mint_weights} represent synaptic connections between
  groups of neurons.
\item A \lstinline{struct mint_network} represents a neural networks.
\item A \lstinline{mint_op} represent an operation that can be
  performed on the previous three types.
\item A \lstinline{mint_spread} holds information on how a network is
  updated.
\end{enumerate}
You will usually deal with nodes, weights, and networks. Occasionally
you will want to work with ops, very rarely you will have to use
spreads in your C code.

Neural network computations are implemented as operations on nodes,
weights, and networks. Once a network is set up, there are three kinds
of operations that are most typical.
\begin{description}
\item[Node input calculations]  
\end{description}


A typical operation is, for instance, to
calculate the output of neurons based on their current input (and
possibly internal state variables). This is called a ``node update'' 

An explanation of operations is
provided WHERE.


The building blocks of a \mint network are:
\begin{description}
\item[Node groups:] They model groups of neurons. They are variables
  of type \lstinline{mint_nodes} and look to the user as
  two-dimensional arrays of \lstinline{float}s. The first index refers
  to a \textit{state variable} of the nodes, the second is the node
  index. Two state variables are automatically created for each node
  group: input and output. Thus, if that \lstinline{n} is a node
  group, \lstinline{n[0][i]} is the input received by the $i$-th node
  and \lstinline{n[1][i]} is its output. Other state variables can be
  created as needed, see typically to endow the nodes with specific
  properties (\fref{sec:transfer-functions}.
\item[Weight matrices:] They model synapses between neurons and other
  ways neurons interact (see \fref{sec:hormones-etc},
  \fref{sec:body-states}). They are variables of type
  \lstinline{mint_weights} and look to the user as three-dimensional
  arrays of \lstinline{float}s. The first index refers to a state
  variable of the weights, the second the row index and the third the
  column index in the weight matrix. One state variable is
  automatically created and represents the weight value. Thus, if
  \lstinline{w} is a weight matrix, \lstinline{w[0][r][c]} is the
  value of the weight in row $r$ and column $c$ of the matrix. Other
  state variables can be created as needed.
\item[Update functions] determine how nodes change their activity in
  response to input, and how weights change their values (how they
  \textit{learn}). Each node group and each weight matrix is given an
  update function, unless we do not need to update them (e.g., fixed
  weights).
\item[Networks] are composed of node groups connected by weight
  matrices, linked together by a \textit{spread scheme}, which is a
  description of the operations that are performed when the network is
  updated. They are variables of type \lstinline{struct mint_network}.
\end{description}

Typically, you will create one or more \mint networks by specifying
the properties of a number of node groups (their size, number of
states, activation function, etc.) and how they are connected via
weight matrices. You may also specify how the network is updated
(synchronouse or asynchronous dynamics, feed-forward spreading of
activation, etc.).


\chapter{Cookbook}
\label{chap:cookbook}

\section{How do i create a feed-forward network?}
\label{sec:feed-forward}

\section{How do I create a recurrent network?}
\label{sec:recurrent}

\section{How do I run a network?}
\label{sec:running-network}

\section{How can a network learn?}
\label{sec:learning}

\section{How do I choose node transfer functions?}
\label{sec:transfer-functions}

\section{How do I set learning rules?}
\label{sec:learning-rules}

\section{How do I set up how a network is updated?}
\label{sec:spreading}

\section{How do I use images as network stimuli?}
\label{sec:images}

\section{How do I simulate a sense organ?}
\label{sec:sense-organ}

\section{How do I simulate hormones, neuromodulators, etc.?}
\label{sec:hormones-etc}

\section{How do I simulate  hunger, thirst, and other body states?}
\label{sec:body-states}

\section{How do I simulate spiking neurons?}
\label{sec:spiking}

\section{How do I simulate continuous time?}
\label{sec:continuous-time}

\section{How do I control a robot or some kind of hardware with a network?}
\label{sec:hardware-control}

\section{How do I visualize networks, node groups and weight
  matrices?}
\label{sec:network-analysis}

\section{Input file syntax}

\subsection{Node model}
\label{sec:nodemodel}
A \mint node is characterized by an input $y$, an output $z$ and
optionally a state vector $\vec s$, with any number of components.
Node inputs may be set by the user (e.g.\ to simulate the reception of
a stimulus) or arise from the weighted outputs of other nodes (via
weight matrices). Node output is determined from node input and node
state by an arbitrary function:
\begin{equation}
  \label{eq:node-model}
  z = f( y, \vec s, \vec g )
\end{equation}
This operation is a called a \textit{node update}, and is typically
performed as part of the \textit{spreading scheme}. The spreading
scheme is the sequence of matrix-vector multiplications and node
update operations whereby network state is updated (see
\fref{sec:spreading}). In addition to affecting node output, an update
may have other effects such as modifying node state.

\subsection{Weight model}
\label{sec:weightmodel}
A weight is characterized by a value $w$ and a state vector $\vec s$,
with 0 or more components. A weight can be updated in much the same
way as a node, though weight update is usually called ``learning.''
In \mint, an arbitrary function can be supplied to update weights
based on current value, current state and the current input, output
and state variables of the nodes joined by a weight.

\section{Node update functions}
\label{sec:nupdate}

\subsection{Using update functions provided with \mint}
\label{sec:update-using}

See \lstinline{update.h} and \lstinline{nlib.h}.

\subsection{Adding update functions}
\label{sec:update-adding}

Adding an update function to \mint requires two steps:
\begin{enumerate}
\item writing the update function itself;
\item letting \mint know about the function.
\end{enumerate}
These operations do not require modifications to the \mint source.
The prototype of a node update function is
\begin{lstlisting}
void function( mint_nodes n, int min, int max );
\end{lstlisting}
where
\begin{itemize}
\item \lstinline{n} is the node object to update;
\item \lstinline{min}-\lstinline{max} is the range of nodes that the
  function should update;
\end{itemize}
The basic structure of an  update rule is very simple:\footnote{The
  \lstinline{for} loop assumes that the same operation is performed to
update all nodes. Different operations, however, may be performed if
desired.}
\begin{lstlisting}
void function( mint_nodes n, int min, int max ) {
  int i;
  /* retrieve update function parameters from node object */
  for( i=min; i<max; i++ ) {
    /* update node i */
  }
}  
\end{lstlisting}
For instance, imagine we want to update nodes as follows: if the input
is above a threshold, the output should be set to 1, otherwise it
should be set to 0. We store the threshold value as parameter 0 of the
rule (we will see below how to tell \mint about the number of
parameters taken by a rule). The implementation of the rule would be:
\begin{lstlisting}
void threshold_node( mint_nodes n, int min, int max, float *p ) {
  int i;
  float threshold;
  threshold = mint_update_get_param( mint_nodes_get_update(n), 0 );
  for( i=min; i<max; i++ ) {
    if( n[0][i] > threshold )
      n[1][i] = 1.;
    else
      n[1][i] = 0.;
  }
}  
\end{lstlisting}
Note that the function specifies an update mechanism (the threshold
mechanism), but does not determine the threshold value itself, which
is stored as a parameter in the node object.  Note also that the
function need not check that \lstinline{min} and \lstinline{max} are
sensible values for this node object, \mint checks this (in debug
mode, see \fref{sec:debugging}).

To be used with \mint, the function must be registered with a call
like:
\begin{lstlisting}
void mint_update_nadd( void (*function)(mint_nodes, int, int, float *),
                       int nstates, int nparam, float *param );
\end{lstlisting}
Thus for the above function:
\begin{lstlisting}
  float p[1] = { 1. };
  mint_update_nadd( threshold_node, 0, 1, p );
\end{lstlisting}
The values provided for the parameter(s) become the default values for
the function, but can be changed as needed (see \lstinline{update.h}).

\section{Spreading schemes}
\label{sec:spreading}

A spreading scheme defines a sequence of matrix-vector multiplications
and node updates.


\section{Debugging}
\label{sec:debugging}

Compiling \mint the \lstinline{DEBUG} flag turns on a number of
checks, ensuring for instance that indices supplied as function
parameters are in the proper range, and that object copies operate on
objects with the same ``geometry'' (e.g.\ node groups with the same
size and number of states).  If a check fails, the program displays a
hopefully informative message and calls \lstinline{abort()}.
\Fref{sec:install} explains how to compile a debugging version of
\mint.


\section{Replacing the stock matrix-vector multiplication}
\label{sec:mvm}

\chapter{Space and speed}
\label{chap:details}

The major efficiency considerations in \mint is not having data
structures for single nodes and weights, but rather for node groups
and weight matrices directly, and caring for the memory layout of
data. The benefits are seen mostly in the function that performs
matrix-vector multiplication. A bare-bones such function would be:
\begin{lstlisting}
void matrix_vector_multiply( mint_weights w, mint_nodes from, mint_nodes to ) {
  int i, j, r, c;
  c = mint_weights_cols( w );
  r = mint_weights_rows( w );
  for( i=0; i<r; i++ ) for( j=0; j<c; j++ ) 
    to[0][i] += w[0][i][j] * from[1][j];
}  
\end{lstlisting}
Due to the memory layout of nodes and weights, all memory accesses are
sequential in the above \lstinline{for} loops. Indeed, the actual code
for matrix-vector multiplication in \mint is slightly longer to take
advantage of this memory layout:\footnote{The \lstinline{float *p}
  argument comes from the general form of \mint ops, and is not used
  here (see REF).}

\lstinputlisting[firstline=36,lastline=50]{../../src/wop.c}

In lines 7, 8, and 10 we set up pointers to the first weight value,
the first target variable of \lstinline{to} nodes,\footnote{By
  default, the target is 0, i.e., node input as in the previous
  listing, but can be changed if desired, see REF.} and the first
output value of \lstinline{from} nodes. Then in the for loops we
simply increment these pointers because we know that all memory
locations are contiguous. The C compiler, on the other hand, cannot do
this because it does not know that, say, \lstinline{w[0][1]} points to
a location that is next to \lstinline{w[0][0][cols]}. Thus the
compiler has to generate additional instructions to do such memory
look-up.

Efficiency considerations have also influenced the design of node and
weight update functions. The approach that first comes to mind to
update a node group is something like (pseudo-code):
\begin{lstlisting}
for( i=0; i<n; i++ )
  update_the_node( node_group_object, i );
\end{lstlisting}
where the function call updates node $i$ of the node group. The
drawback is that there are $n$ function calls. In \mint, an update
function takes a \textit{range} as argument, telling it which nodes to
update. Thus the loop is inside the update function, saving $n-1$
function calls. In other words, in \mint we have 
(pseudo-code):
\begin{lstlisting}
  update_node_range( node_group_object, imin, imax );
\end{lstlisting}
where \lstinline{update_node_range()} does both the looping and the
node updating.\footnote{The update function cannot be inlined because
  it can be configured by the user, hence the compiler cannot know
  which function to inline.} The same approach is taken for weight
updates.

\section{Benchmarks}

\clearpage

\printindex

\end{document}

